//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	matmul
.extern .shared .align 16 .b8 cache[];

.visible .entry matmul(
	.param .u64 matmul_param_0,
	.param .u64 matmul_param_1,
	.param .u64 matmul_param_2,
	.param .u32 matmul_param_3,
	.param .u32 matmul_param_4,
	.param .u32 matmul_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd18, [matmul_param_0];
	ld.param.u64 	%rd19, [matmul_param_1];
	ld.param.u64 	%rd17, [matmul_param_2];
	ld.param.u32 	%r14, [matmul_param_3];
	ld.param.u32 	%r12, [matmul_param_4];
	ld.param.u32 	%r13, [matmul_param_5];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %ctaid.x;
	mov.u32 	%r20, %tid.x;
	mad.lo.s32 	%r2, %r19, %r18, %r20;
	setp.ge.s32 	%p1, %r1, %r14;
	setp.ge.s32 	%p2, %r2, %r13;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_9;

	setp.lt.s32 	%p4, %r12, 1;
	mov.f32 	%f29, 0f00000000;
	@%p4 bra 	$L__BB0_8;

	add.s32 	%r22, %r12, -1;
	and.b32  	%r31, %r12, 3;
	setp.lt.u32 	%p5, %r22, 3;
	mov.f32 	%f29, 0f00000000;
	mov.u32 	%r30, 0;
	@%p5 bra 	$L__BB0_5;

	sub.s32 	%r29, %r12, %r31;
	mul.wide.s32 	%rd20, %r2, 4;
	add.s64 	%rd32, %rd1, %rd20;
	mul.lo.s32 	%r24, %r12, %r1;
	mul.wide.s32 	%rd21, %r24, 4;
	add.s64 	%rd22, %rd2, %rd21;
	add.s64 	%rd31, %rd22, 8;
	mul.wide.s32 	%rd5, %r13, 4;

$L__BB0_4:
	ld.global.f32 	%f12, [%rd32];
	ld.global.f32 	%f13, [%rd31+-8];
	fma.rn.f32 	%f14, %f13, %f12, %f29;
	add.s64 	%rd23, %rd32, %rd5;
	ld.global.f32 	%f15, [%rd23];
	ld.global.f32 	%f16, [%rd31+-4];
	fma.rn.f32 	%f17, %f16, %f15, %f14;
	add.s64 	%rd24, %rd23, %rd5;
	ld.global.f32 	%f18, [%rd24];
	ld.global.f32 	%f19, [%rd31];
	fma.rn.f32 	%f20, %f19, %f18, %f17;
	add.s64 	%rd25, %rd24, %rd5;
	add.s64 	%rd32, %rd25, %rd5;
	ld.global.f32 	%f21, [%rd25];
	ld.global.f32 	%f22, [%rd31+4];
	fma.rn.f32 	%f29, %f22, %f21, %f20;
	add.s32 	%r30, %r30, 4;
	add.s64 	%rd31, %rd31, 16;
	add.s32 	%r29, %r29, -4;
	setp.ne.s32 	%p6, %r29, 0;
	@%p6 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB0_8;

	mad.lo.s32 	%r25, %r30, %r13, %r2;
	mul.wide.s32 	%rd26, %r25, 4;
	add.s64 	%rd34, %rd1, %rd26;
	mul.wide.s32 	%rd11, %r13, 4;
	mad.lo.s32 	%r26, %r12, %r1, %r30;
	mul.wide.s32 	%rd27, %r26, 4;
	add.s64 	%rd33, %rd2, %rd27;

$L__BB0_7:
	.pragma "nounroll";
	ld.global.f32 	%f23, [%rd34];
	ld.global.f32 	%f24, [%rd33];
	fma.rn.f32 	%f29, %f24, %f23, %f29;
	add.s64 	%rd34, %rd34, %rd11;
	add.s64 	%rd33, %rd33, 4;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB0_7;

$L__BB0_8:
	mad.lo.s32 	%r27, %r1, %r13, %r2;
	cvta.to.global.u64 	%rd28, %rd17;
	mul.wide.s32 	%rd29, %r27, 4;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.f32 	[%rd30], %f29;

$L__BB0_9:
	ret;

}
	// .globl	transpose
.visible .entry transpose(
	.param .u64 transpose_param_0,
	.param .u64 transpose_param_1,
	.param .u32 transpose_param_2,
	.param .u32 transpose_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_param_0];
	ld.param.u64 	%rd2, [transpose_param_1];
	ld.param.u32 	%r3, [transpose_param_2];
	ld.param.u32 	%r4, [transpose_param_3];
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r11, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r11, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mad.lo.s32 	%r12, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB1_2:
	ret;

}
	// .globl	elementwise_sigmoid
.visible .entry elementwise_sigmoid(
	.param .u64 elementwise_sigmoid_param_0,
	.param .u32 elementwise_sigmoid_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [elementwise_sigmoid_param_0];
	ld.param.u32 	%r2, [elementwise_sigmoid_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	neg.f32 	%f2, %f1;
	mov.f32 	%f3, 0f3F000000;
	mov.f32 	%f4, 0f3BBB989D;
	fma.rn.f32 	%f5, %f2, %f4, %f3;
	mov.f32 	%f6, 0f3FB8AA3B;
	mov.f32 	%f7, 0f437C0000;
	cvt.sat.f32.f32 	%f8, %f5;
	mov.f32 	%f9, 0f4B400001;
	fma.rm.f32 	%f10, %f8, %f7, %f9;
	add.f32 	%f11, %f10, 0fCB40007F;
	neg.f32 	%f12, %f11;
	fma.rn.f32 	%f13, %f2, %f6, %f12;
	mov.f32 	%f14, 0f32A57060;
	fma.rn.f32 	%f15, %f2, %f14, %f13;
	mov.b32 	%r6, %f10;
	shl.b32 	%r7, %r6, 23;
	mov.b32 	%f16, %r7;
	ex2.approx.ftz.f32 	%f17, %f15;
	fma.rn.f32 	%f18, %f17, %f16, 0f3F800000;
	rcp.rn.f32 	%f19, %f18;
	st.global.f32 	[%rd4], %f19;

$L__BB2_2:
	ret;

}
	// .globl	elementwise_sub
.visible .entry elementwise_sub(
	.param .u64 elementwise_sub_param_0,
	.param .u64 elementwise_sub_param_1,
	.param .u64 elementwise_sub_param_2,
	.param .u32 elementwise_sub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [elementwise_sub_param_0];
	ld.param.u64 	%rd2, [elementwise_sub_param_1];
	ld.param.u64 	%rd3, [elementwise_sub_param_2];
	ld.param.u32 	%r2, [elementwise_sub_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

$L__BB3_2:
	ret;

}
	// .globl	axpy
.visible .entry axpy(
	.param .f32 axpy_param_0,
	.param .u64 axpy_param_1,
	.param .u64 axpy_param_2,
	.param .u32 axpy_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.f32 	%f1, [axpy_param_0];
	ld.param.u64 	%rd1, [axpy_param_1];
	ld.param.u64 	%rd2, [axpy_param_2];
	ld.param.u32 	%r2, [axpy_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f2, [%rd6];
	add.s64 	%rd7, %rd3, %rd5;
	ld.global.f32 	%f3, [%rd7];
	fma.rn.f32 	%f4, %f2, %f1, %f3;
	st.global.f32 	[%rd7], %f4;

$L__BB4_2:
	ret;

}
	// .globl	dot_product
.visible .entry dot_product(
	.param .u64 dot_product_param_0,
	.param .u64 dot_product_param_1,
	.param .u64 dot_product_param_2,
	.param .u32 dot_product_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [dot_product_param_0];
	ld.param.u64 	%rd4, [dot_product_param_1];
	ld.param.u64 	%rd5, [dot_product_param_2];
	ld.param.u32 	%r12, [dot_product_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r18, %r2, %r1, %r3;
	setp.ge.s32 	%p1, %r18, %r12;
	mov.f32 	%f13, 0f00000000;
	@%p1 bra 	$L__BB5_3;

	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r5, %r13, %r1;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;

$L__BB5_2:
	mul.wide.s32 	%rd6, %r18, 4;
	add.s64 	%rd7, %rd1, %rd6;
	add.s64 	%rd8, %rd2, %rd6;
	ld.global.f32 	%f6, [%rd8];
	ld.global.f32 	%f7, [%rd7];
	fma.rn.f32 	%f13, %f7, %f6, %f13;
	add.s32 	%r18, %r18, %r5;
	setp.lt.s32 	%p2, %r18, %r12;
	@%p2 bra 	$L__BB5_2;

$L__BB5_3:
	shl.b32 	%r14, %r3, 2;
	mov.u32 	%r15, cache;
	add.s32 	%r8, %r15, %r14;
	st.shared.f32 	[%r8], %f13;
	bar.sync 	0;
	shr.u32 	%r19, %r1, 1;
	setp.eq.s32 	%p3, %r19, 0;
	@%p3 bra 	$L__BB5_8;

$L__BB5_5:
	setp.ge.s32 	%p4, %r3, %r19;
	@%p4 bra 	$L__BB5_7;

	shl.b32 	%r16, %r19, 2;
	add.s32 	%r17, %r8, %r16;
	ld.shared.f32 	%f8, [%r8];
	ld.shared.f32 	%f9, [%r17];
	add.f32 	%f10, %f9, %f8;
	st.shared.f32 	[%r8], %f10;

$L__BB5_7:
	bar.sync 	0;
	shr.s32 	%r11, %r19, 1;
	setp.gt.s32 	%p5, %r19, 1;
	mov.u32 	%r19, %r11;
	@%p5 bra 	$L__BB5_5;

$L__BB5_8:
	setp.ne.s32 	%p6, %r3, 0;
	@%p6 bra 	$L__BB5_10;

	ld.shared.f32 	%f11, [cache];
	cvta.to.global.u64 	%rd9, %rd5;
	mul.wide.u32 	%rd10, %r2, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f11;

$L__BB5_10:
	ret;

}
	// .globl	elementwise_log
.visible .entry elementwise_log(
	.param .u64 elementwise_log_param_0,
	.param .u32 elementwise_log_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [elementwise_log_param_0];
	ld.param.u32 	%r2, [elementwise_log_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB6_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f5, [%rd1];
	add.f32 	%f6, %f5, 0f3089705F;
	setp.lt.f32 	%p2, %f6, 0f00800000;
	mul.f32 	%f7, %f6, 0f4B000000;
	selp.f32 	%f1, %f7, %f6, %p2;
	selp.f32 	%f8, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	%r6, %f1;
	add.s32 	%r7, %r6, -1059760811;
	and.b32  	%r8, %r7, -8388608;
	sub.s32 	%r9, %r6, %r8;
	mov.b32 	%f9, %r9;
	cvt.rn.f32.s32 	%f10, %r8;
	mov.f32 	%f11, 0f34000000;
	fma.rn.f32 	%f12, %f10, %f11, %f8;
	add.f32 	%f13, %f9, 0fBF800000;
	mov.f32 	%f14, 0f3E1039F6;
	mov.f32 	%f15, 0fBE055027;
	fma.rn.f32 	%f16, %f15, %f13, %f14;
	mov.f32 	%f17, 0fBDF8CDCC;
	fma.rn.f32 	%f18, %f16, %f13, %f17;
	mov.f32 	%f19, 0f3E0F2955;
	fma.rn.f32 	%f20, %f18, %f13, %f19;
	mov.f32 	%f21, 0fBE2AD8B9;
	fma.rn.f32 	%f22, %f20, %f13, %f21;
	mov.f32 	%f23, 0f3E4CED0B;
	fma.rn.f32 	%f24, %f22, %f13, %f23;
	mov.f32 	%f25, 0fBE7FFF22;
	fma.rn.f32 	%f26, %f24, %f13, %f25;
	mov.f32 	%f27, 0f3EAAAA78;
	fma.rn.f32 	%f28, %f26, %f13, %f27;
	mov.f32 	%f29, 0fBF000000;
	fma.rn.f32 	%f30, %f28, %f13, %f29;
	mul.f32 	%f31, %f13, %f30;
	fma.rn.f32 	%f32, %f31, %f13, %f13;
	mov.f32 	%f33, 0f3F317218;
	fma.rn.f32 	%f36, %f12, %f33, %f32;
	setp.lt.u32 	%p3, %r6, 2139095040;
	@%p3 bra 	$L__BB6_3;

	mov.f32 	%f34, 0f7F800000;
	fma.rn.f32 	%f36, %f1, %f34, %f34;

$L__BB6_3:
	setp.eq.f32 	%p4, %f1, 0f00000000;
	selp.f32 	%f35, 0fFF800000, %f36, %p4;
	st.global.f32 	[%rd1], %f35;

$L__BB6_4:
	ret;

}
	// .globl	cost_kernel
.visible .entry cost_kernel(
	.param .u64 cost_kernel_param_0,
	.param .u64 cost_kernel_param_1,
	.param .u64 cost_kernel_param_2,
	.param .u64 cost_kernel_param_3,
	.param .u32 cost_kernel_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [cost_kernel_param_0];
	ld.param.u64 	%rd2, [cost_kernel_param_1];
	ld.param.u64 	%rd3, [cost_kernel_param_2];
	ld.param.u64 	%rd4, [cost_kernel_param_3];
	ld.param.u32 	%r2, [cost_kernel_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB7_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd7];
	mov.f32 	%f3, 0f3F800000;
	sub.f32 	%f4, %f3, %f2;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f5, [%rd11];
	mul.f32 	%f6, %f4, %f5;
	fma.rn.f32 	%f7, %f2, %f1, %f6;
	neg.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	st.global.f32 	[%rd13], %f8;

$L__BB7_2:
	ret;

}
	// .globl	sum_reduction
.visible .entry sum_reduction(
	.param .u64 sum_reduction_param_0,
	.param .u64 sum_reduction_param_1,
	.param .u32 sum_reduction_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [sum_reduction_param_0];
	ld.param.u64 	%rd3, [sum_reduction_param_1];
	ld.param.u32 	%r12, [sum_reduction_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r18, %r2, %r1, %r3;
	setp.ge.s32 	%p1, %r18, %r12;
	mov.f32 	%f12, 0f00000000;
	@%p1 bra 	$L__BB8_3;

	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r5, %r13, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB8_2:
	mul.wide.s32 	%rd4, %r18, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f6, [%rd5];
	add.f32 	%f12, %f12, %f6;
	add.s32 	%r18, %r18, %r5;
	setp.lt.s32 	%p2, %r18, %r12;
	@%p2 bra 	$L__BB8_2;

$L__BB8_3:
	shl.b32 	%r14, %r3, 2;
	mov.u32 	%r15, cache;
	add.s32 	%r8, %r15, %r14;
	st.shared.f32 	[%r8], %f12;
	bar.sync 	0;
	shr.u32 	%r19, %r1, 1;
	setp.eq.s32 	%p3, %r19, 0;
	@%p3 bra 	$L__BB8_8;

$L__BB8_5:
	setp.ge.s32 	%p4, %r3, %r19;
	@%p4 bra 	$L__BB8_7;

	shl.b32 	%r16, %r19, 2;
	add.s32 	%r17, %r8, %r16;
	ld.shared.f32 	%f7, [%r8];
	ld.shared.f32 	%f8, [%r17];
	add.f32 	%f9, %f8, %f7;
	st.shared.f32 	[%r8], %f9;

$L__BB8_7:
	bar.sync 	0;
	shr.s32 	%r11, %r19, 1;
	setp.gt.s32 	%p5, %r19, 1;
	mov.u32 	%r19, %r11;
	@%p5 bra 	$L__BB8_5;

$L__BB8_8:
	setp.ne.s32 	%p6, %r3, 0;
	@%p6 bra 	$L__BB8_10;

	ld.shared.f32 	%f10, [cache];
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

$L__BB8_10:
	ret;

}

