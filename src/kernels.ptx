//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_75
.address_size 64

	// .globl	matmul_tiled
// _ZZ12matmul_tiledE2As has been demoted
// _ZZ12matmul_tiledE2Bs has been demoted
.extern .shared .align 16 .b8 cache[];

.visible .entry matmul_tiled(
	.param .u64 matmul_tiled_param_0,
	.param .u64 matmul_tiled_param_1,
	.param .u64 matmul_tiled_param_2,
	.param .u32 matmul_tiled_param_3,
	.param .u32 matmul_tiled_param_4,
	.param .u32 matmul_tiled_param_5
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<63>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<16>;
	// demoted variable
	.shared .align 4 .b8 _ZZ12matmul_tiledE2As[1024];
	// demoted variable
	.shared .align 4 .b8 _ZZ12matmul_tiledE2Bs[1024];

	ld.param.u64 	%rd4, [matmul_tiled_param_0];
	ld.param.u64 	%rd5, [matmul_tiled_param_1];
	ld.param.u64 	%rd6, [matmul_tiled_param_2];
	ld.param.u32 	%r20, [matmul_tiled_param_3];
	ld.param.u32 	%r21, [matmul_tiled_param_4];
	ld.param.u32 	%r22, [matmul_tiled_param_5];
	mov.u32 	%r23, %ctaid.y;
	shl.b32 	%r24, %r23, 4;
	mov.u32 	%r41, %tid.y;
	add.s32 	%r2, %r24, %r41;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r3, %r25, 4;
	mov.u32 	%r40, %tid.x;
	add.s32 	%r5, %r3, %r40;
	setp.lt.s32 	%p1, %r21, 1;
	mov.f32 	%f62, 0f00000000;
	@%p1 bra 	$L__BB0_7;

	shl.b32 	%r27, %r41, 6;
	mov.u32 	%r28, _ZZ12matmul_tiledE2As;
	add.s32 	%r8, %r28, %r27;
	shl.b32 	%r29, %r40, 2;
	add.s32 	%r6, %r8, %r29;
	mov.u32 	%r30, _ZZ12matmul_tiledE2Bs;
	add.s32 	%r31, %r30, %r27;
	add.s32 	%r7, %r31, %r29;
	add.s32 	%r9, %r30, %r29;
	mad.lo.s32 	%r32, %r41, %r22, %r40;
	add.s32 	%r42, %r32, %r3;
	mad.lo.s32 	%r33, %r21, %r2, %r40;
	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r33, 4;
	add.s64 	%rd15, %rd7, %rd8;
	add.s32 	%r34, %r21, 15;
	shr.s32 	%r35, %r34, 31;
	shr.u32 	%r36, %r35, 28;
	add.s32 	%r37, %r34, %r36;
	shr.s32 	%r11, %r37, 4;
	mov.u32 	%r43, 0;
	cvta.to.global.u64 	%rd9, %rd5;

$L__BB0_2:
	setp.ge.s32 	%p2, %r40, %r21;
	setp.ge.s32 	%p3, %r2, %r20;
	mov.f32 	%f61, 0f00000000;
	or.pred  	%p4, %p3, %p2;
	mov.f32 	%f60, %f61;
	@%p4 bra 	$L__BB0_4;

	ld.global.f32 	%f60, [%rd15];

$L__BB0_4:
	st.shared.f32 	[%r6], %f60;
	setp.ge.s32 	%p5, %r41, %r21;
	setp.ge.s32 	%p6, %r5, %r22;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB0_6;

	mul.wide.s32 	%rd10, %r42, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f32 	%f61, [%rd11];

$L__BB0_6:
	st.shared.f32 	[%r7], %f61;
	bar.sync 	0;
	ld.shared.f32 	%f12, [%r9];
	ld.shared.f32 	%f13, [%r8];
	fma.rn.f32 	%f14, %f13, %f12, %f62;
	ld.shared.f32 	%f15, [%r9+64];
	ld.shared.f32 	%f16, [%r8+4];
	fma.rn.f32 	%f17, %f16, %f15, %f14;
	ld.shared.f32 	%f18, [%r9+128];
	ld.shared.f32 	%f19, [%r8+8];
	fma.rn.f32 	%f20, %f19, %f18, %f17;
	ld.shared.f32 	%f21, [%r9+192];
	ld.shared.f32 	%f22, [%r8+12];
	fma.rn.f32 	%f23, %f22, %f21, %f20;
	ld.shared.f32 	%f24, [%r9+256];
	ld.shared.f32 	%f25, [%r8+16];
	fma.rn.f32 	%f26, %f25, %f24, %f23;
	ld.shared.f32 	%f27, [%r9+320];
	ld.shared.f32 	%f28, [%r8+20];
	fma.rn.f32 	%f29, %f28, %f27, %f26;
	ld.shared.f32 	%f30, [%r9+384];
	ld.shared.f32 	%f31, [%r8+24];
	fma.rn.f32 	%f32, %f31, %f30, %f29;
	ld.shared.f32 	%f33, [%r9+448];
	ld.shared.f32 	%f34, [%r8+28];
	fma.rn.f32 	%f35, %f34, %f33, %f32;
	ld.shared.f32 	%f36, [%r9+512];
	ld.shared.f32 	%f37, [%r8+32];
	fma.rn.f32 	%f38, %f37, %f36, %f35;
	ld.shared.f32 	%f39, [%r9+576];
	ld.shared.f32 	%f40, [%r8+36];
	fma.rn.f32 	%f41, %f40, %f39, %f38;
	ld.shared.f32 	%f42, [%r9+640];
	ld.shared.f32 	%f43, [%r8+40];
	fma.rn.f32 	%f44, %f43, %f42, %f41;
	ld.shared.f32 	%f45, [%r9+704];
	ld.shared.f32 	%f46, [%r8+44];
	fma.rn.f32 	%f47, %f46, %f45, %f44;
	ld.shared.f32 	%f48, [%r9+768];
	ld.shared.f32 	%f49, [%r8+48];
	fma.rn.f32 	%f50, %f49, %f48, %f47;
	ld.shared.f32 	%f51, [%r9+832];
	ld.shared.f32 	%f52, [%r8+52];
	fma.rn.f32 	%f53, %f52, %f51, %f50;
	ld.shared.f32 	%f54, [%r9+896];
	ld.shared.f32 	%f55, [%r8+56];
	fma.rn.f32 	%f56, %f55, %f54, %f53;
	ld.shared.f32 	%f57, [%r9+960];
	ld.shared.f32 	%f58, [%r8+60];
	fma.rn.f32 	%f62, %f58, %f57, %f56;
	bar.sync 	0;
	shl.b32 	%r38, %r22, 4;
	add.s32 	%r42, %r42, %r38;
	add.s32 	%r41, %r41, 16;
	add.s64 	%rd15, %rd15, 64;
	add.s32 	%r40, %r40, 16;
	add.s32 	%r43, %r43, 1;
	setp.lt.s32 	%p8, %r43, %r11;
	@%p8 bra 	$L__BB0_2;

$L__BB0_7:
	setp.ge.s32 	%p9, %r5, %r22;
	setp.ge.s32 	%p10, %r2, %r20;
	or.pred  	%p11, %p10, %p9;
	@%p11 bra 	$L__BB0_9;

	mad.lo.s32 	%r39, %r2, %r22, %r5;
	cvta.to.global.u64 	%rd12, %rd6;
	mul.wide.s32 	%rd13, %r39, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f62;

$L__BB0_9:
	ret;

}
	// .globl	transpose
.visible .entry transpose(
	.param .u64 transpose_param_0,
	.param .u64 transpose_param_1,
	.param .u32 transpose_param_2,
	.param .u32 transpose_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_param_0];
	ld.param.u64 	%rd2, [transpose_param_1];
	ld.param.u32 	%r3, [transpose_param_2];
	ld.param.u32 	%r4, [transpose_param_3];
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ntid.y;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r11, %r1, %r4, %r2;
	mul.wide.s32 	%rd4, %r11, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mad.lo.s32 	%r12, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB1_2:
	ret;

}
	// .globl	elementwise_sigmoid
.visible .entry elementwise_sigmoid(
	.param .u64 elementwise_sigmoid_param_0,
	.param .u32 elementwise_sigmoid_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [elementwise_sigmoid_param_0];
	ld.param.u32 	%r2, [elementwise_sigmoid_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	neg.f32 	%f2, %f1;
	mov.f32 	%f3, 0f3F000000;
	mov.f32 	%f4, 0f3BBB989D;
	fma.rn.f32 	%f5, %f2, %f4, %f3;
	mov.f32 	%f6, 0f3FB8AA3B;
	mov.f32 	%f7, 0f437C0000;
	cvt.sat.f32.f32 	%f8, %f5;
	mov.f32 	%f9, 0f4B400001;
	fma.rm.f32 	%f10, %f8, %f7, %f9;
	add.f32 	%f11, %f10, 0fCB40007F;
	neg.f32 	%f12, %f11;
	fma.rn.f32 	%f13, %f2, %f6, %f12;
	mov.f32 	%f14, 0f32A57060;
	fma.rn.f32 	%f15, %f2, %f14, %f13;
	mov.b32 	%r6, %f10;
	shl.b32 	%r7, %r6, 23;
	mov.b32 	%f16, %r7;
	ex2.approx.ftz.f32 	%f17, %f15;
	fma.rn.f32 	%f18, %f17, %f16, 0f3F800000;
	rcp.rn.f32 	%f19, %f18;
	st.global.f32 	[%rd4], %f19;

$L__BB2_2:
	ret;

}
	// .globl	elementwise_sub
.visible .entry elementwise_sub(
	.param .u64 elementwise_sub_param_0,
	.param .u64 elementwise_sub_param_1,
	.param .u64 elementwise_sub_param_2,
	.param .u32 elementwise_sub_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [elementwise_sub_param_0];
	ld.param.u64 	%rd2, [elementwise_sub_param_1];
	ld.param.u64 	%rd3, [elementwise_sub_param_2];
	ld.param.u32 	%r2, [elementwise_sub_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd6];
	sub.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f3;

$L__BB3_2:
	ret;

}
	// .globl	axpy
.visible .entry axpy(
	.param .f32 axpy_param_0,
	.param .u64 axpy_param_1,
	.param .u64 axpy_param_2,
	.param .u32 axpy_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.f32 	%f1, [axpy_param_0];
	ld.param.u64 	%rd1, [axpy_param_1];
	ld.param.u64 	%rd2, [axpy_param_2];
	ld.param.u32 	%r2, [axpy_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f2, [%rd6];
	add.s64 	%rd7, %rd3, %rd5;
	ld.global.f32 	%f3, [%rd7];
	fma.rn.f32 	%f4, %f2, %f1, %f3;
	st.global.f32 	[%rd7], %f4;

$L__BB4_2:
	ret;

}
	// .globl	sum_of_squares_reduction
.visible .entry sum_of_squares_reduction(
	.param .u64 sum_of_squares_reduction_param_0,
	.param .u64 sum_of_squares_reduction_param_1,
	.param .u32 sum_of_squares_reduction_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [sum_of_squares_reduction_param_0];
	ld.param.u64 	%rd3, [sum_of_squares_reduction_param_1];
	ld.param.u32 	%r12, [sum_of_squares_reduction_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r18, %r2, %r1, %r3;
	setp.ge.s32 	%p1, %r18, %r12;
	mov.f32 	%f12, 0f00000000;
	@%p1 bra 	$L__BB5_3;

	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r5, %r13, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB5_2:
	mul.wide.s32 	%rd4, %r18, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f6, [%rd5];
	fma.rn.f32 	%f12, %f6, %f6, %f12;
	add.s32 	%r18, %r18, %r5;
	setp.lt.s32 	%p2, %r18, %r12;
	@%p2 bra 	$L__BB5_2;

$L__BB5_3:
	shl.b32 	%r14, %r3, 2;
	mov.u32 	%r15, cache;
	add.s32 	%r8, %r15, %r14;
	st.shared.f32 	[%r8], %f12;
	bar.sync 	0;
	shr.u32 	%r19, %r1, 1;
	setp.eq.s32 	%p3, %r19, 0;
	@%p3 bra 	$L__BB5_8;

$L__BB5_5:
	setp.ge.s32 	%p4, %r3, %r19;
	@%p4 bra 	$L__BB5_7;

	shl.b32 	%r16, %r19, 2;
	add.s32 	%r17, %r8, %r16;
	ld.shared.f32 	%f7, [%r8];
	ld.shared.f32 	%f8, [%r17];
	add.f32 	%f9, %f8, %f7;
	st.shared.f32 	[%r8], %f9;

$L__BB5_7:
	bar.sync 	0;
	shr.s32 	%r11, %r19, 1;
	setp.gt.s32 	%p5, %r19, 1;
	mov.u32 	%r19, %r11;
	@%p5 bra 	$L__BB5_5;

$L__BB5_8:
	setp.ne.s32 	%p6, %r3, 0;
	@%p6 bra 	$L__BB5_10;

	ld.shared.f32 	%f10, [cache];
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

$L__BB5_10:
	ret;

}
	// .globl	proximal_update_l1
.visible .entry proximal_update_l1(
	.param .u64 proximal_update_l1_param_0,
	.param .f32 proximal_update_l1_param_1,
	.param .u32 proximal_update_l1_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [proximal_update_l1_param_0];
	ld.param.f32 	%f2, [proximal_update_l1_param_1];
	ld.param.u32 	%r2, [proximal_update_l1_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB6_6;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	setp.gt.f32 	%p2, %f1, %f2;
	@%p2 bra 	$L__BB6_5;
	bra.uni 	$L__BB6_2;

$L__BB6_5:
	sub.f32 	%f5, %f1, %f2;
	st.global.f32 	[%rd1], %f5;
	bra.uni 	$L__BB6_6;

$L__BB6_2:
	neg.f32 	%f3, %f2;
	setp.lt.f32 	%p3, %f1, %f3;
	@%p3 bra 	$L__BB6_4;
	bra.uni 	$L__BB6_3;

$L__BB6_4:
	add.f32 	%f4, %f1, %f2;
	st.global.f32 	[%rd1], %f4;
	bra.uni 	$L__BB6_6;

$L__BB6_3:
	mov.u32 	%r6, 0;
	st.global.u32 	[%rd1], %r6;

$L__BB6_6:
	ret;

}
	// .globl	dot_product
.visible .entry dot_product(
	.param .u64 dot_product_param_0,
	.param .u64 dot_product_param_1,
	.param .u64 dot_product_param_2,
	.param .u32 dot_product_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [dot_product_param_0];
	ld.param.u64 	%rd4, [dot_product_param_1];
	ld.param.u64 	%rd5, [dot_product_param_2];
	ld.param.u32 	%r12, [dot_product_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r18, %r2, %r1, %r3;
	setp.ge.s32 	%p1, %r18, %r12;
	mov.f32 	%f13, 0f00000000;
	@%p1 bra 	$L__BB7_3;

	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r5, %r13, %r1;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;

$L__BB7_2:
	mul.wide.s32 	%rd6, %r18, 4;
	add.s64 	%rd7, %rd1, %rd6;
	add.s64 	%rd8, %rd2, %rd6;
	ld.global.f32 	%f6, [%rd8];
	ld.global.f32 	%f7, [%rd7];
	fma.rn.f32 	%f13, %f7, %f6, %f13;
	add.s32 	%r18, %r18, %r5;
	setp.lt.s32 	%p2, %r18, %r12;
	@%p2 bra 	$L__BB7_2;

$L__BB7_3:
	shl.b32 	%r14, %r3, 2;
	mov.u32 	%r15, cache;
	add.s32 	%r8, %r15, %r14;
	st.shared.f32 	[%r8], %f13;
	bar.sync 	0;
	shr.u32 	%r19, %r1, 1;
	setp.eq.s32 	%p3, %r19, 0;
	@%p3 bra 	$L__BB7_8;

$L__BB7_5:
	setp.ge.s32 	%p4, %r3, %r19;
	@%p4 bra 	$L__BB7_7;

	shl.b32 	%r16, %r19, 2;
	add.s32 	%r17, %r8, %r16;
	ld.shared.f32 	%f8, [%r8];
	ld.shared.f32 	%f9, [%r17];
	add.f32 	%f10, %f9, %f8;
	st.shared.f32 	[%r8], %f10;

$L__BB7_7:
	bar.sync 	0;
	shr.s32 	%r11, %r19, 1;
	setp.gt.s32 	%p5, %r19, 1;
	mov.u32 	%r19, %r11;
	@%p5 bra 	$L__BB7_5;

$L__BB7_8:
	setp.ne.s32 	%p6, %r3, 0;
	@%p6 bra 	$L__BB7_10;

	ld.shared.f32 	%f11, [cache];
	cvta.to.global.u64 	%rd9, %rd5;
	mul.wide.u32 	%rd10, %r2, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f11;

$L__BB7_10:
	ret;

}
	// .globl	elementwise_log
.visible .entry elementwise_log(
	.param .u64 elementwise_log_param_0,
	.param .u32 elementwise_log_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [elementwise_log_param_0];
	ld.param.u32 	%r2, [elementwise_log_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB8_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f5, [%rd1];
	add.f32 	%f6, %f5, 0f3089705F;
	setp.lt.f32 	%p2, %f6, 0f00800000;
	mul.f32 	%f7, %f6, 0f4B000000;
	selp.f32 	%f1, %f7, %f6, %p2;
	selp.f32 	%f8, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	%r6, %f1;
	add.s32 	%r7, %r6, -1059760811;
	and.b32  	%r8, %r7, -8388608;
	sub.s32 	%r9, %r6, %r8;
	mov.b32 	%f9, %r9;
	cvt.rn.f32.s32 	%f10, %r8;
	mov.f32 	%f11, 0f34000000;
	fma.rn.f32 	%f12, %f10, %f11, %f8;
	add.f32 	%f13, %f9, 0fBF800000;
	mov.f32 	%f14, 0f3E1039F6;
	mov.f32 	%f15, 0fBE055027;
	fma.rn.f32 	%f16, %f15, %f13, %f14;
	mov.f32 	%f17, 0fBDF8CDCC;
	fma.rn.f32 	%f18, %f16, %f13, %f17;
	mov.f32 	%f19, 0f3E0F2955;
	fma.rn.f32 	%f20, %f18, %f13, %f19;
	mov.f32 	%f21, 0fBE2AD8B9;
	fma.rn.f32 	%f22, %f20, %f13, %f21;
	mov.f32 	%f23, 0f3E4CED0B;
	fma.rn.f32 	%f24, %f22, %f13, %f23;
	mov.f32 	%f25, 0fBE7FFF22;
	fma.rn.f32 	%f26, %f24, %f13, %f25;
	mov.f32 	%f27, 0f3EAAAA78;
	fma.rn.f32 	%f28, %f26, %f13, %f27;
	mov.f32 	%f29, 0fBF000000;
	fma.rn.f32 	%f30, %f28, %f13, %f29;
	mul.f32 	%f31, %f13, %f30;
	fma.rn.f32 	%f32, %f31, %f13, %f13;
	mov.f32 	%f33, 0f3F317218;
	fma.rn.f32 	%f36, %f12, %f33, %f32;
	setp.lt.u32 	%p3, %r6, 2139095040;
	@%p3 bra 	$L__BB8_3;

	mov.f32 	%f34, 0f7F800000;
	fma.rn.f32 	%f36, %f1, %f34, %f34;

$L__BB8_3:
	setp.eq.f32 	%p4, %f1, 0f00000000;
	selp.f32 	%f35, 0fFF800000, %f36, %p4;
	st.global.f32 	[%rd1], %f35;

$L__BB8_4:
	ret;

}
	// .globl	cost_kernel
.visible .entry cost_kernel(
	.param .u64 cost_kernel_param_0,
	.param .u64 cost_kernel_param_1,
	.param .u64 cost_kernel_param_2,
	.param .u64 cost_kernel_param_3,
	.param .u32 cost_kernel_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [cost_kernel_param_0];
	ld.param.u64 	%rd2, [cost_kernel_param_1];
	ld.param.u64 	%rd3, [cost_kernel_param_2];
	ld.param.u64 	%rd4, [cost_kernel_param_3];
	ld.param.u32 	%r2, [cost_kernel_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB9_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd7];
	mov.f32 	%f3, 0f3F800000;
	sub.f32 	%f4, %f3, %f2;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f5, [%rd11];
	mul.f32 	%f6, %f4, %f5;
	fma.rn.f32 	%f7, %f2, %f1, %f6;
	neg.f32 	%f8, %f7;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	st.global.f32 	[%rd13], %f8;

$L__BB9_2:
	ret;

}
	// .globl	sum_reduction
.visible .entry sum_reduction(
	.param .u64 sum_reduction_param_0,
	.param .u64 sum_reduction_param_1,
	.param .u32 sum_reduction_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [sum_reduction_param_0];
	ld.param.u64 	%rd3, [sum_reduction_param_1];
	ld.param.u32 	%r12, [sum_reduction_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r18, %r2, %r1, %r3;
	setp.ge.s32 	%p1, %r18, %r12;
	mov.f32 	%f12, 0f00000000;
	@%p1 bra 	$L__BB10_3;

	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r5, %r13, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB10_2:
	mul.wide.s32 	%rd4, %r18, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f6, [%rd5];
	add.f32 	%f12, %f12, %f6;
	add.s32 	%r18, %r18, %r5;
	setp.lt.s32 	%p2, %r18, %r12;
	@%p2 bra 	$L__BB10_2;

$L__BB10_3:
	shl.b32 	%r14, %r3, 2;
	mov.u32 	%r15, cache;
	add.s32 	%r8, %r15, %r14;
	st.shared.f32 	[%r8], %f12;
	bar.sync 	0;
	shr.u32 	%r19, %r1, 1;
	setp.eq.s32 	%p3, %r19, 0;
	@%p3 bra 	$L__BB10_8;

$L__BB10_5:
	setp.ge.s32 	%p4, %r3, %r19;
	@%p4 bra 	$L__BB10_7;

	shl.b32 	%r16, %r19, 2;
	add.s32 	%r17, %r8, %r16;
	ld.shared.f32 	%f7, [%r8];
	ld.shared.f32 	%f8, [%r17];
	add.f32 	%f9, %f8, %f7;
	st.shared.f32 	[%r8], %f9;

$L__BB10_7:
	bar.sync 	0;
	shr.s32 	%r11, %r19, 1;
	setp.gt.s32 	%p5, %r19, 1;
	mov.u32 	%r19, %r11;
	@%p5 bra 	$L__BB10_5;

$L__BB10_8:
	setp.ne.s32 	%p6, %r3, 0;
	@%p6 bra 	$L__BB10_10;

	ld.shared.f32 	%f10, [cache];
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

$L__BB10_10:
	ret;

}

